# M√©todo dos gradientes conjugados

## Introdu√ß√£o

O m√©todo dos gradientes conjugados (daqui para frente abreviado como CG, da sigla em ingl√™s) √© o m√©todo iterativo mais popular para resolver sistemas grandes de equa√ß√µes lineares. Esse m√©todo √© particularmente efetivo para resolver sistemas $$Ax = b$$ onde $x$ √© um vetor desconhecido (a inc√≥gnita), $b$ √© conhecido e $A$ √© uma matriz positiva-definida. Esse m√©todo √© mais adequado para quando $A$ for esparsa: quando $A$ √© densa, provavelmente um m√©todo mais adequado seria alguma fatora√ß√£o (como a $LU$); mas quando $A$ √© esparsa, suas fatora√ß√µes geralmente s√£o densas, impondo um custo de mem√≥ria que pode ser inaceit√°vel.

Apesar de CG ser usado para resolver sistemas de equa√ß√µes lineares, um melhor entendimento sobre o m√©todo vem ao considerarmos um problema de otimiza√ß√£o quadr√°tica 
$$
min \quad f(x) = \frac12(x,x)_A - (b,x) + c
$$\tag{1}

onde $(x,y)_A := (x,Ay)$ e $(x,y)$ √© o produto interno usual. Temos que a derivada de $f$ em $x$, $D(f)(x)$, √© $\frac12 (A'+ A)x - b$, que, quando $A$ √© sim√©trica, √© $Ax - b$. O m√≠nimo (ou m√°ximo) $x^*$ da fun√ß√£o √© obtido onde sua derivada se anula, do que segue que resolvemos para$Ax^* - b = 0$, rendendo o sistema linear inicial. Se $A$, al√©m de sim√©trica, √© positiva-definida, $x^*$ √© um m√≠nimo. Se $A$ n√£o for sim√©trica, CG vai buscar solu√ß√£o para $\frac12(A' + A)x = b$ (note que $A' + A$ √© sim√©trica).

Se $A$ √© positiva-definida, sua forma bilinear correspondente $(x,A'x)$ (ou melhor, o seu gr√°fico) corresponde a um parabol√≥ide, do que √© intuitivo que $x^*$ √© √∫nico. 

## Ideia geral do m√©todo

A ideia b√°sica desse m√©todo (assim como a de outros m√©todos) √©

1. Come√ßando de um $x_0$;
2. Escolha uma dire√ß√£o d_i;
3. Escolha quanto se quer "andar" na dire√ß√£o $d_i$, determinando o tamanho de passo $\alpha$;
4. Fa√ßa $x_i := x_{i-1} + \alpha d_i$.

Assim, podemos definir os vetores

* Res√≠duo: $r_i := b - Ax_i$;
* Erro: $e_i := x_i - x$ (note que $r_i = Ae_i$)

Note que $r_i = - D(f)(x_i)$, que √© a dire√ß√£o em $x_i$ que aponta onde a fun√ß√£o est√° diminuindo mais rapidamente - que, o que √© importante, √© ortogonal √† curva de n√≠vel em $x_i$. 

O m√©todo de m√°xima descida, no qual CG se baseia, faz $d_i = r_{i-1}$, e busca um passo $\alpha$ que minimize a fun√ß√£o nessa dire√ß√£o. Deve ser intuitivo que essa minimiza√ß√£o ocorre quando $\alpha$ for tal que fa√ßa $r_{i-1} \perp r_i$ (de modo que $x_{i-1} + \alpha d_i$ tangencie uma curva de n√≠vel de $f$). Uma conta r√°pida mostra que $\alpha = \frac{(r_{i-1},r_{i-1})}{(r_{i-1},r_{i-1})_A}$.

O m√©todo de m√°xima descida no geral leva mais de uma itera√ß√£o para encontrar a solu√ß√£o. Na verdade, m√°xima descida converge em apenas uma itera√ß√£o apenas se o chute inicial $x_0$ estiver na dire√ß√£o de um dos auto-vetores de $A$, que coincide com um dos eixos do elips√≥ide (o que raramente ocorre, a n√£o ser que $A$ seja m√∫ltipla da identidade, fazendo com que as curvas de n√≠vel de sua forma bilinear sejam esferas, e seus auto-valores sejam todos iguais).



### Gradientes conjugados

A ideia do m√©todo de gradientes conjugados √© buscar pela solu√ß√£o iterativamente por $n$ dire√ß√µes de busca $\{d_0, \ldots, d_n\}$, de modo que se possa garantir que $x_i$ seja a melhor solu√ß√£o para o problema em $\mathcal D_i = span(\{d_0, \ldots, d_i\})$.

### Conjuga√ß√£o de Gram-Schmidt

Em particular, fazemos as dire√ß√µes de busca $d_i$ A-ortogonais (ou _conjugados_) entre si, ie. $(d_i,d_j)_A = 0$ se $i \neq j$. Podemos pensar na A-ortogonalidade como uma ortogonalidade usual sob uma mudan√ßa de coordenadas que transforma os elips√≥ides das curvas de n√≠vel em esferas, que √© do que se deriva boa parte das propriedades que obtemos de CG.

Para obter essas dire√ß√µes, podemos prosseguir pelo processo de Gram-Schmidt, mas tomando o produto interno como sendo o A-produto interno $(\cdot,\cdot)_A$, descrito brevemente a seguir:

Tome um conjunto de $n$ vetores linearmente independentes $\{u_i\}$. Para obter $d_i$ bastar tomar os $u_i$, em ordem, e subtrair os componentes que n√£o sejam A-ortogonais com os $d_i$ anteriores. Esse √© um algoritmo incremental:

$$
\begin{split}
    d_0 &= u_0 \\
    \text{para i > 0} \quad d_i &= u_i - \sum_{k=0}^{i-1}\beta_{ik}d_k \\
    \beta_{ij} &= \frac{(u_i, d_j)_A}{(d_j,d_j)_A}
\end{split}
$$


Em CG, faz-se $u_i = r_i$. Como $r_{i+1} = r_i - \alpha_i Ad_i$, temos que cada $r_i$ √© uma combina√ß√£o linear do res√≠duo anterior e $Ad_{i-1}$. Disso, obtemos ent√£o que $\mathcal D_i$, o "espa√ßo de busca" at√© o passo i, √© formado pela uni√£o de $\mathcal D_i$ e $A \mathcal D_i$, do que segue que $$\mathcal D_i = span\{d_0, Ad_0, A^2d_0, \ldots, A^{i-1}d_0\} = span\{r_0, Ar_0, A^2r_0, \ldots, A^{i-1}r_0\}. $$ Esses espa√ßos s√£o chamados espa√ßos de Krylov (mais em geral, $Krylov_r(A,b) := \{A^jb|j<r\}$). 

Disso tiramos a propriedade de que, como $r_{i+1}$ √© ortogonal a $\mathcal D_{i+1}$ (o que segue do fato de o res√≠duo em qualquer ponto ser ortogonal √° superf√≠cie elipsoidal naquele ponto, e de o hiperplano $x_0 + \mathcal D_i$ tamb√©m o ser), $r_{i+1}$ √© A-ortogonal a $\mathcal D_i \subset D_{i+1}$. Assim, $r_{i+1}$ √© A-ortogonal a todas as dire√ß√µes de busca anteriores, exceto $d_i$, o que facilita grandemente o processo de Gram-Schmidt descrito acima.

Mais especificamente, obtemos que 
$$
(r_i, d_j)_A = \begin{cases} \frac1{\alpha_i} (r_i,r_i), & i = j \\
                            -\frac1{\alpha_{i-1}} (r_i,r_i), & i = j + 1\\ 
                              0, & \text{caso contr√°rio}
                              \end{cases}
$$

o que simplifica grandemente a express√£o de $\beta_{ij}$, levando a complexidade (tanto em tempo quanto em espa√ßo) de CG de $O(n^2)$ para $O(m)$, onde m √© a quantidade de elementos n√£o nulos de $A$. Mais especificamente, obtemos
$$
\beta_i := \beta_{i,i-1} = \frac{(r_i,r_i)}{(r_{i-1},r_{i-1})}
$$

### Precondicionador

Dada uma matriz $A$ cujos maior e menor autovalores s√£o, respectivamente, $\lambda_1$ e $\lambda_n$, definimos seu _n√∫mero de condi√ß√£o_, ou simplesmente seu _condicionante_ como $\kappa(A) := \frac{\lambda_1}{\lambda_n}$. O condicionante de uma matriz tem um papel importante na velocidade de converg√™ncia de diversos m√©todos e, em particular, para CG. Em geral valores grandes para $\kappa$ s√£o ruins, fazendo com que o m√©todo possa levar a uma converg√™ncia inaceitavelmente lenta.

Para lidar com esse problema, uma alternativa √© a pre-multiplica√ß√£o do sistema por uma matriz $M$ positiva-definida que aproxima $A$, e que seja de f√°cil invers√£o. Ent√£o, pode-se resolver $Ax = b$ de forma indireta, fazendo
$$
M^{-1}Ax = M^{-1}b
$$
Se $\kappa(M^{-1}A) \ll \kappa(A)$, o problema acima se torna muito mais f√°cil do que o problema original. 

O uso de precondicionadores apresenta, contudo, alguns problemas de ordem pr√°tica e te√≥rica. Um problema √© que, mesmo $A$ e $M$ sendo sim√©tricas e definidas, $M^{-1}A$ pode n√£o ser. Isso pode ser contornado, usando a decomposi√ß√£o de Cholesky, obtendo $E$ tal que $EE' = M$, e $E^{-1}AE^{-1}'$ ser√° uma sim√©trica e positiva-definida.

Qual precondicionador usar √© um problema de ordem pr√°tica, existindo algumas op√ß√µes comuns, a depender do problema em m√£os. Uma op√ß√£o simples, mas de resultados mediocres, √© o precondicionador de Jacobi, em que $M$ √© uma matriz diagonal (realmente, muito f√°cil de inverter). Outra op√ß√£o √© precondicionamento de Cholesky incompleto, em que √© feita uma decomposi√ß√£o de Cholesky incompleta da matriz A

Mas vale dizer que √©, em geral, entendido que CG √© sempre feito com o uso de algum precondicionador, ao menos para problemas de grandes, o que motiva a n√£o-omiss√£o desse tema neste documento introdut√≥rio.

## Algoritmo

O m√©todo de gradientes conjugados pode ent√£o ser resumido no seguinte algoritmo (em que foram feitas algumas manipula√ß√µes alg√©bricas para comportar o condicionador $M$, sem a necessidade de computar $E$).

Dados $A$, $b$, um valor inicial para $x$ uma quantidade m√°xima de itera√ß√µes $i_{max}$ e toler√¢ncia de erro $\epsilon < 1$ e um precondicionador $M$,

$$
\begin{split}
 &i \leftarrow 0 \\
 &r \leftarrow b - Ax \\
 &d \leftarrow M^{-1}r \\
 &\delta_{novo} \leftarrow (r,d) \\
 &\delta_0 \leftarrow \delta_{novo} \\
 & \text{Enquanto}\quad i < i_{max}\quad e \quad\delta_{novo} > \epsilon^2\delta_0 \quad\text{fa√ßa }\\
 & \qquad q \leftarrow Ad \\
 & \qquad \alpha \leftarrow \frac{\delta_{novo}}{(d,q)} \\
 & \qquad x \leftarrow x + \alpha d \\
 & \qquad r \leftarrow r - \alpha q \\
 & \qquad s \leftarrow M^{-1}r \\
 & \qquad \delta_{velho} \leftarrow \delta_{novo} \\
 & \qquad \delta_{novo} \leftarrow (r,s) \\
 & \qquad \beta \leftarrow \delta_{novo}/\delta_{velho} \\
 & \qquad d \leftarrow s + \beta d \\
 & \qquad i \leftarrow i + 1
\end{split}
$$

Apesar de, a princ√≠pio, o m√©todo convergir em $n$ itera√ß√µes ($n$ sendo a dimens√£o do problema), na pr√°tica isso pode n√£o ocorrer devido a instabilidade num√©rica - mas, mesmo deixando instabilidade num√©rica de lado, para muitos problemas rodar $n$ itera√ß√µes √© computacionalmente impratic√°vel. Al√©m disso, em v√°rios casos CG chega muito pr√≥ximo √† solu√ß√£o uma quantidade $k \ll n$ de itera√ß√µes.

Por esses motivos s√£o necess√°rios os par√¢metros $imax$ e $\epsilon$.

### Implementa√ß√£o

Para fins de exemplo, veja como esse algoritmo pode ser implementado em Julia


```julia
using LinearAlgebra # para podermos usar I, a identidade, por conveni√™ncia e sem perda de efici√™ncia computacional
function CG(A, b, x0, imax = size(A)[1], Minv = I, œµ = 1e-5)
    i  = 0
    x  = x0
    r  = b - A*x
    d  = Minv*r
    Œ¥n = r'd     
    Œ¥0 = Œ¥n
    while i < imax && Œ¥n > œµ^2*Œ¥0
        q  = A*d
        Œ±  = Œ¥n/(d'q)
        x  = x + Œ±*d
        if (i + 1) √∑ 50 == 0  # por quest√£o de estabilidade num√©rica, recalculamos r a cada 50 itera√ß√µes
            r = b - A*x
        else
            r  = r - Œ±*q
        end
        s  = Minv*r
        Œ¥v = Œ¥n       # Œ¥ velho
        Œ¥n = r's      # Œ¥ novo
        Œ≤  = Œ¥n/Œ¥v
        d  = s + Œ≤*d
        i += 1
    end
    return x
end
```




    CG (generic function with 4 methods)



#### Teste simples


```julia
A  = [3 2; 2 6]

b  = [2; -8]
x0 = [14; -20]

x = CG(A, b, x0)
```




    2-element Array{Float64,1}:
      2.0
     -2.000000000000001



Vemos que, ao menos para esse problema pequeno (para _sanity check_), o m√©todo funciona bem, apesar de haver a adi√ß√£o de um valor esp√∫rio da ordem de $10^{-15}$:


```julia
A\b
```




    2-element Array{Float64,1}:
      2.0
     -2.0



#### CG n√£o linear (NCG)

CG pode ser usado para encontrar o m√≠nimo de quaisquer fun√ß√µes cont√≠nuas $f$, com algumas mudan√ßas:

1. a f√≥rmula para o res√≠duo muda;
2. √© mais dif√≠cil calcular $\alpha$ - √© preciso fazer, por exemplo, uma busca linear;
3. existem diferentes escolhas poss√≠veis para $\beta$.

N√£o entraremos em muitos detalhes aqui, mas vale fazer uma compara√ß√£o com o popular BFGS. BFGS √© um m√©todo quase-Newton bem conhecido, cuja principal caracter√≠stica que afeta sua performance (em particular, em armazenamento) √© a utiliza√ß√£o de uma aproxima√ß√£o da Hessiana de $f$. Para problemas grandes, essa o armazenamento dessa aproxima√ß√£o pode significar um grande custo computacional, tornando sua utiliza√ß√£o invi√°vel, e o NCG uma alternativa v√°lida.

Se a mem√≥ria n√£o for um problema, no entanto, BFGS, exceto para casos espec√≠ficos, tende a se tornar uma melhor op√ß√£o (na m√©dia, uma itera√ß√£o de BFGS equivale a $n$ de NCG em quest√£o de converg√™ncia, de modo que mesmo uma itera√ß√£o de NCG podendo ser mais barata do que uma de BFGS, no geral essa diferen√ßa n√£o compensa).

A variante L-BFGS (l√™-se "_limited memory BFGS_") do BFGS √© uma aproxima√ß√£o do mesmo que apresenta menor consumo de mem√≥ria e (assintoticamente) menor tempo computacional por itera√ß√£o, apesar de converg√™ncia mais lenta.

Vale lembrar que essas s√£o considera√ß√µes gerais, no entanto, sendo que cada problema pr√°tico pode exigir uma an√°lise mais detalhada, apontando qual m√©todo seria mais efetivo.

## Exemplo de Aplica√ß√£o: Resolvendo o problema de Poisson em elementos finitos

O problema de Poisson √© o de encontrar $u$ tal que
$$
\begin{split}
\nabla^2 u(x) &= -f,\quad & x \in \Omega \\
u(x) &= u_D(x),\quad &x \in \partial\Omega.
\end{split}
$$

Onde $u = u(x)$ √© a fun√ß√£o desconhecida, $f = f(x)$ √© uma fun√ß√£o, $\nabla^2$ √© o operador de Laplace, $\Omega$ √© o dom√≠nio espacial e $\partial\Omega$ √© sua borda.

Apesar de simples, esse problema √© muito importante, por exemplo, para aplica√ß√µes em f√≠sica e engenharia. 

Para prosseguir pelo m√©todo de elementos finitos, precisamos reescrever o problema em sua forma variacional. Para tanto, primeiro multiplica-se a equa√ß√£o por uma fun√ß√£o $v$, dita _fun√ß√£o de teste_, e integramos no dom√≠nio
$$
\int_{\Omega} (\nabla^2u)v dx = \int_{\Omega} fv dx
$$

ent√£o, aplicando integra√ß√£o por partes e insistindo que $v$ deve sumir na borda, obt√©m-se o sistema

$$
\int_{\Omega}\nabla u \cdot \nabla v dx = \int_{\Omega} fv dx \quad \forall v \in V
$$

ou, de forma mais compacta (em que se colocam os problemas para serem resolvidos por m√©todos elementos finitos no geral)

$$
a(u,v) = l(v)
$$

onde a forma bilinear $a$ √© $\int_{\Omega}\nabla u \cdot \nabla v dx$ e a forma linear $v$ √© $\int_{\Omega} fv dx$, para $v in V$, onde $V$ √© chamado "espa√ßo de teste" (costuma-se insistir que $V$ seja um espa√ßo Sobolev apropriado, mas por quest√µes de brevidade vamos omitir aqui esse tipo de detalhe t√©cnico).

A solu√ß√£o $u$ da EDP subjacente deve estar em um espa√ßo de fun√ß√µes com derivadas cont√≠nuas, mas o espa√ßo de Sobolev posto pela formula√ß√£o variacional permite variadas descont√≠nuas, o que tem grandes consequ√™ncias pr√°ticas, como permitir a constru√ß√£o de uma solu√ß√£o a partir da "colagem" de fun√ß√µes polinomiais por partes.

Para resolver o problema, introduzimos uma discretiza√ß√£o e buscaremos uma aproxima√ß√£o poligonal $u_N$ para $u$. O espa√ßo $\Omega$ √© discretizado em uma malha conforme (assumiremos aqui a discretiza√ß√£o em um complexo simplicial, mas outras discretiza√ß√µes s√£o poss√≠veis). Assim, fazemos $V_N = \{v \in C^0 : v|_{s_i} \text{√© linear e } v(\partial \Omega) = 0 \}$, sendo $s_i$ os simplexos da discretiza√ß√£o. $V_N$ tem dimens√£o finita, sendo $v \in V_N$ unicamente determinado por seus valores nos pontos da malha.

Dada uma base $\{\phi_i\}$ para $V_N$ (por exemplo, a base de Lagrange usual), temos $u_N = \sum_1^N U_j\phi_j$. Como todo $v \in V_N$ √© combina√ß√£o linear dos $\{\phi_j\}$, vemos que a formula√ß√£o variacional √© equivalente a

$$
\int_{\Omega} u_N' \phi_k' dx = \int_{\Omega} f \phi_k dx \text{ para } k = 1,\ldots, N
$$

do que segue que
$$
\sum_1^N U_j\int_{\Omega} \phi_j'\phi_k' dx = \int_{\Omega} f \phi_k dx \text{ para } k = 1,\ldots, N
$$

Assim, o problema se torn encontrar $U \in \mathbb R^N$ resolvendo o sistema 
$$
AU = F
$$

onde $a_{ij} = \int_{\Omega} \phi_j' \phi_i' dx$ e $F_k = \int_{\Omega} f \phi_k dx$.

Tipicamente, a matriz $A$ √© altamente esparsa quando se escolhe uma base apropriada (geralmente polin√¥mios de Lagrange) e, quando se busca solu√ß√£o para uma malha de refinamento alto, o problema pode atingir dimens√µes muito altas (e especialmente para malhas em $\mathbb R^3$). Assim, para problemas grandes o suficiente (que s√£o na verdade problemas de tamanho modesto para malhas em $\mathbb R^3$), √© necess√°rio o uso de m√©todos iterativos, entre os quais CG √© entre os mais populares. CG √© especialmente adequado para o problema de Poisson, que resulta em uma matriz sim√©trica e positiva-definida, e pacotes para m√©todos de elementos finitos populares, como o [fenics](https://fenicsproject.org/) oferecem a op√ß√£o de us√°-lo como m√©todo iterativo (com precondicionador apropriado).

## Experimentos computacionais

A seguir s√£o feitos experimentos computacionais quanto √† adequa√ß√£o de CG para a resolu√ß√£o de sistemas esparsos, com ou sem condicionador.

Para efeitos dos experimentos, foi utilizada uma matriz de Wathen.  Uma matriz de Wathen(Nx,Ny) √© uma matriz de elementos finitos aleat√≥ria N por N (fazendo $N = 3NxNy + 2Nx + 2Ny + 1$), sendo a "matriz de consist√™ncia de massa" para um grade regular Nx por Ny de 8 elementos nodais em 2 dimens√µes espaciais. A matriz √© sim√©trica positiva definida para qualquer valor (positivo) da densidade, que √© escolhida aleatoriamente.


```julia
using BenchmarkTools, MatrixDepot, IterativeSolvers, LinearAlgebra, SparseArrays

# Matriz de Wathen de dimens√µes 30401 x 30401
A = matrixdepot("wathen", 100)
```

    include group.jl for user defined matrix generators
    verify download of index files...
    reading database
    adding metadata...
    adding svd data...
    writing database
    used remote sites are sparse.tamu.edu with MAT index and math.nist.gov with HTML index





    30401√ó30401 SparseMatrixCSC{Float64,Int64} with 471601 stored entries:
      [1    ,     1]  =  6.2388
      [2    ,     1]  =  -6.2388
      [3    ,     1]  =  2.0796
      [202  ,     1]  =  -6.2388
      [203  ,     1]  =  -8.31839
      [303  ,     1]  =  2.0796
      [304  ,     1]  =  -8.31839
      [305  ,     1]  =  3.1194
      [1    ,     2]  =  -6.2388
      [2    ,     2]  =  33.2736
      [3    ,     2]  =  -6.2388
      [202  ,     2]  =  20.796
      ‚ãÆ
      [30199, 30400]  =  21.3736
      [30200, 30400]  =  21.3736
      [30399, 30400]  =  -6.41209
      [30400, 30400]  =  34.1978
      [30401, 30400]  =  -6.41209
      [30097, 30401]  =  3.20604
      [30098, 30401]  =  -8.54945
      [30099, 30401]  =  2.13736
      [30199, 30401]  =  -8.54945
      [30200, 30401]  =  -6.41209
      [30399, 30401]  =  2.13736
      [30400, 30401]  =  -6.41209
      [30401, 30401]  =  6.41209




```julia
using UnicodePlots
spy(A)
```




    [1m                      Sparsity Pattern[22m
    [90m         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê[39m    
           [90m1[39m[90m ‚îÇ[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m [31m> 0[39m
            [90m ‚îÇ[39m[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m [34m< 0[39m
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[90m‚îÇ[39m    
            [90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[35m‚°Ä[39m[0m‚†Ä[90m‚îÇ[39m    
       [90m30401[39m[90m ‚îÇ[39m[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[0m‚†Ä[35m‚†à[39m[35m‚†ª[39m[35m‚£¶[39m[90m‚îÇ[39m    
    [90m         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò[39m    
    [90m         1[39m[90m                    [39m[90m                  30401[39m
    [0m                         nz = 471601




```julia
# N√≠vel de esparsidade
count(!iszero, A) / length(A)
```




    0.0005102687577359558




```julia
b = ones(size(A, 1))
# Resolve Ax=b by CG
xcg = cg(A, b);
@benchmark cg($A, $b)
```




    BenchmarkTools.Trial: 
      memory estimate:  951.36 KiB
      allocs estimate:  16
      --------------
      minimum time:     280.710 ms (0.00% GC)
      median time:      430.303 ms (0.00% GC)
      mean time:        438.680 ms (0.00% GC)
      maximum time:     765.398 ms (0.00% GC)
      --------------
      samples:          12
      evals/sample:     1



### Usando precondicionador de Cholesky


```julia
using Preconditioners
@time p = CholeskyPreconditioner(A, 2)
```

      4.076178 seconds (2.43 M allocations: 151.649 MiB, 1.26% gc time)





    CholeskyPreconditioner{Float64,SparseMatrixCSC{Float64,Int64}}([7.7740167657825285 0.0 ‚Ä¶ 0.0 0.0; 0.0 11.520622371156024 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 3.048358439214821 0.0; 0.0 0.0 ‚Ä¶ 0.0 5.7811090761367625], 2)



###### Resolve Ax=b com precondicionador


```julia
xpcg = cg(A, b, Pl=p)
# same answer?
norm(xcg - xpcg)
```




    5.306315159734449e-7



##### CG foi, neste exemplo > vezes 10 mais lento do que CG
O que √© curioso, porque parece que em outros computadores o resultado √© o inverso (CG com precondicionador (PCG) √© > 10 vezes mais r√°pido).


```julia
@benchmark cg($A, $b, Pl=$p)
```




    BenchmarkTools.Trial: 
      memory estimate:  951.36 KiB
      allocs estimate:  16
      --------------
      minimum time:     4.958 s (0.00% GC)
      median time:      5.235 s (0.00% GC)
      mean time:        5.235 s (0.00% GC)
      maximum time:     5.513 s (0.00% GC)
      --------------
      samples:          2
      evals/sample:     1



## Refer√™ncias

Para escrever este documento, foram usadas, primariamente, as seguintes refer√™ncias:

- An Introduction to the Conjugate Gradient Method Without the Agonizing Pain, de Jonathan Richard Shewchuk, primariamente para o desenvolvimento te√≥rico [dispon√≠vel aqui](http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) 
- Notas de aula de Hua-Zhou, primariamente para a implementa√ß√£o computacional [dispon√≠vel aqui](http://hua-zhou.github.io/teaching/biostatm280-2019spring/slides/16-cg/cg.html)
- Documenta√ß√£o da fun√ß√£o Wathen para a sua utiliza√ß√£o [dispon√≠vel aqui](http://www.netlib.org/templates/mltemplates.v1_1/wathen.m) (foi utilizada a documenta√ß√£o da fun√ß√£o mas n√£o a fun√ß√£o indicada, que foi feita para MatLab)
- Galerkin Approximations and Finite Element Methods, de Ricardo G. Dur√°n, para alguns detalhes quanto a m√©todos de elementos finitos [dispon√≠vel aqui](http://mate.dm.uba.ar/~rduran/class_notes/fem.pdf)
- Solving PDEs in Python - The FEniCS Tutorial Volume 1, de Hans Peter Langtangen e Anders Logg, para outros detalhes quanto a m√©todos de elementos finitos para o problema de Poisson, [dispon√≠vel aqui](https://fenicsproject.org/pub/tutorial/pdf/fenics-tutorial-vol1.pdf)
